{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd78e466",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 3.1 Introduction to Web Scraping & Ethics\n",
    "\n",
    "Before we dive into the technicalities, it's vital to understand *what* web scraping is and, more importantly, the *ethical and legal boundaries* surrounding it. This isn't just about good manners; it's about avoiding potential legal trouble and ensuring you don't overwhelm websites with your requests.\n",
    "\n",
    "-----\n",
    "\n",
    "### üìö Table of Contents: Web Scraping with Beautiful Soup\n",
    "\n",
    "  * **3.1 Introduction to Web Scraping & Ethics** üï∏Ô∏è\n",
    "      * What is Web Scraping? (Definition, Use Cases) ü§î\n",
    "      * Ethical Considerations (robots.txt, Terms of Service, Rate Limiting) ‚öñÔ∏è\n",
    "      * Legal Implications of Scraping üìú\n",
    "  * **3.2 HTTP Requests with `requests` library** üåê\n",
    "      * Making GET and POST Requests ‚û°Ô∏è\n",
    "      * Handling Headers, Parameters, and Cookies ‚öôÔ∏è\n",
    "      * Error Handling (Status Codes) ‚ö†Ô∏è\n",
    "  * **3.3 Beautiful Soup Fundamentals** üç≤\n",
    "      * What is Beautiful Soup? (Parsing HTML/XML) üìÑ\n",
    "      * Installation and Basic Usage üíª\n",
    "      * Creating a `BeautifulSoup` Object üèóÔ∏è\n",
    "      * Navigating the Parse Tree (Tags, Names, Attributes, Strings) üå≥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880015d2",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "### What is Web Scraping? (Definition, Use Cases) ü§î\n",
    "\n",
    "**Web scraping** (also called web harvesting or web data extraction) is the process of automatically extracting data from websites. Instead of manually copying and pasting information, a web scraper uses code to read and collect data, transforming unstructured web data into structured data (like CSV files or databases).\n",
    "\n",
    "Imagine trying to get the prices of 100 different products from an e-commerce site every day. Doing it manually would be tedious and error-prone. A web scraper can do this automatically and much faster\\!\n",
    "\n",
    "**Common Use Cases:**\n",
    "\n",
    "  * **Price Comparison:** Collecting product prices from various e-commerce sites to find the best deals.\n",
    "  * **Market Research:** Gathering data on competitor products, trends, or customer reviews.\n",
    "  * **Lead Generation:** Extracting contact information (if publicly available and ethically permissible) for sales and marketing.\n",
    "  * **News Aggregation:** Collecting articles from multiple news sources on a specific topic.\n",
    "  * **Content Migration:** Moving content from an old website to a new one.\n",
    "  * **Academic Research:** Collecting data for linguistic analysis, social science studies, or scientific data sets.\n",
    "  * **Real Estate Analysis:** Scraping property listings, prices, and features.\n",
    "\n",
    "In essence, if you need data that's visible on a website but not available via a public API, web scraping is often the solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe7e92",
   "metadata": {},
   "source": [
    "\n",
    "### Ethical Considerations (robots.txt, Terms of Service, Rate Limiting) ‚öñÔ∏è\n",
    "\n",
    "This is arguably the most critical part of web scraping. Just because you *can* scrape a website doesn't mean you *should*.\n",
    "\n",
    "#### `robots.txt`\n",
    "\n",
    "The `robots.txt` file is a standard text file that website owners use to communicate with web crawlers and other web robots (like your scraper). It tells robots which parts of the site they are *allowed* or *disallowed* to access.\n",
    "\n",
    "  * **How to check:** To find a website's `robots.txt`, simply append `/robots.txt` to the base URL (e.g., `https://www.example.com/robots.txt`).\n",
    "  * **Respect it:** Always read and respect the directives in `robots.txt`. If it says `Disallow: /private_data`, do not scrape from `/private_data`. Ignoring `robots.txt` is considered unethical and can lead to your IP being blocked.\n",
    "\n",
    "#### Terms of Service (ToS)\n",
    "\n",
    "Most websites have a \"Terms of Service\" or \"Terms and Conditions\" page. This document outlines the legal agreement between the website and its users.\n",
    "\n",
    "  * **Read it:** The ToS often contains clauses regarding data collection, automated access, or scraping. Many websites explicitly forbid scraping.\n",
    "  * **Consequences:** Violating the ToS can lead to legal action, account termination, or IP bans.\n",
    "\n",
    "#### Rate Limiting & Server Load\n",
    "\n",
    "When you make requests to a website, you're using its server resources.\n",
    "\n",
    "  * **Be gentle:** Don't send too many requests in a short period. This is called \"hammering\" a server and can be interpreted as a Denial-of-Service (DoS) attack, even if unintended.\n",
    "  * **Introduce delays:** Use `time.sleep()` between your requests to mimic human Browse behavior and reduce the load on the server.\n",
    "  * **Randomize delays:** Instead of a fixed `time.sleep(1)`, use `time.sleep(np.random.uniform(2, 5))` for more natural pauses.\n",
    "  * **Caching:** Store data you've already scraped to avoid re-requesting it unnecessarily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2567ae8",
   "metadata": {},
   "source": [
    "\n",
    "### Legal Implications of Scraping üìú\n",
    "\n",
    "The legal landscape of web scraping is complex and varies by jurisdiction. There have been numerous high-profile legal cases involving web scraping, with outcomes often depending on the specifics of the data being scraped, how it's used, and the website's terms of service.\n",
    "\n",
    "Key legal concepts that often come into play:\n",
    "\n",
    "  * **Copyright:** Is the data you're scraping copyrighted? Are you infringing on that copyright by copying it?\n",
    "  * **Trespass to Chattel:** This legal theory has been used to argue that excessive scraping can be a \"trespass\" on the website's servers, causing damage or interference.\n",
    "  * **Breach of Contract:** Violating a website's Terms of Service can be considered a breach of contract.\n",
    "  * **Computer Fraud and Abuse Act (CFAA) in the US:** This act, primarily aimed at hacking, has sometimes been invoked in scraping cases, particularly when access is obtained without authorization or exceeds authorized access.\n",
    "  * **Data Protection Regulations (e.g., GDPR in Europe, CCPA in California):** If you're scraping personal data, you must comply with relevant data protection laws. This is a very serious consideration.\n",
    "\n",
    "**Key Takeaways for Ethical and Legal Scraping:**\n",
    "\n",
    "  * **Always check `robots.txt` first.**\n",
    "  * **Read the website's Terms of Service.**\n",
    "  * **Be polite: Introduce delays and avoid overloading servers.**\n",
    "  * **Don't scrape sensitive or private information.**\n",
    "  * **Consider the purpose:** Why do you need this data? Is there an API available?\n",
    "  * **Consult legal counsel:** If you plan large-scale or commercial scraping, it's wise to seek legal advice.\n",
    "\n",
    "**Remember:** Ethical scraping means acting like a good internet citizen. Respect the website's wishes and resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f447c",
   "metadata": {},
   "source": [
    "\n",
    "#### ‚ùì Quick Quiz: Web Scraping Introduction & Ethics\n",
    "\n",
    "1.  Which file is typically checked by web scrapers to determine which parts of a website they are allowed or disallowed to access?\n",
    "\n",
    "      * A) `sitemap.xml`\n",
    "      * B) `index.html`\n",
    "      * C) `robots.txt`\n",
    "      * D) `config.json`\n",
    "\n",
    "2.  What is a good practice to prevent your web scraper from overloading a website's server?\n",
    "\n",
    "      * A) Sending all requests simultaneously.\n",
    "      * B) Ignoring the `robots.txt` file.\n",
    "      * C) Introducing random delays between requests.\n",
    "      * D) Requesting data from only one page.\n",
    "\n",
    "3.  True or False: If a website does not have a `robots.txt` file, it means you can freely scrape any part of it without ethical or legal concerns.\n",
    "\n",
    "*(Answers are at the end of the section\\!)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b717749",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "# 3.2 HTTP Requests with `requests` library\n",
    "\n",
    "To get the HTML content of a webpage, your Python script needs to act like a web browser and make an **HTTP request**. The `requests` library is the de facto standard for making HTTP requests in Python. It's user-friendly, powerful, and handles many complexities for you.\n",
    "\n",
    "You'll need to install it if you haven't already:\n",
    "`pip install requests`\n",
    "\n",
    "### Making GET and POST Requests ‚û°Ô∏è\n",
    "\n",
    "The two most common types of HTTP requests you'll encounter in web scraping are `GET` and `POST`.\n",
    "\n",
    "  * **GET request:** Used to *retrieve* data from a specified resource. When you type a URL into your browser, it sends a GET request. It's typically used for fetching static web pages, images, or data from APIs.\n",
    "  * **POST request:** Used to *send* data to a server to create or update a resource. For example, submitting a form on a website (like login credentials or search queries) often sends a POST request.\n",
    "\n",
    "#### GET Request Example\n",
    "\n",
    "Let's fetch the HTML content of a simple webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8654df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making a GET request to: https://httpbin.org/get\n",
      "Status Code: 200\n",
      "Content Type: application/json\n",
      "\n",
      "Response Body (first 200 chars):\n",
      " {\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.32.3\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-6\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of a public test page (e.g., from Python Requests documentation)\n",
    "url_get = \"https://httpbin.org/get\" # This service echoes back your GET request\n",
    "\n",
    "print(f\"Making a GET request to: {url_get}\")\n",
    "try:\n",
    "    response = requests.get(url_get)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"Content Type: {response.headers['Content-Type']}\")\n",
    "\n",
    "    # The content of the response\n",
    "    # .text for string content (usually HTML, JSON, XML)\n",
    "    # .content for binary content (images, files)\n",
    "    print(\"\\nResponse Body (first 200 chars):\\n\", response.text[:200])\n",
    "\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(f\"Http Error: {errh}\")\n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print(f\"Error Connecting: {errc}\")\n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print(f\"Timeout Error: {errt}\")\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(f\"Something went wrong: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be05200",
   "metadata": {},
   "source": [
    "\n",
    "#### POST Request Example\n",
    "\n",
    "`POST` requests are used when you need to send data to the server, often in the body of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2389cdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making a POST request to: https://httpbin.org/post with data: {'name': 'Alice', 'age': 30, 'city': 'New York'}\n",
      "Status Code: 200\n",
      "\n",
      "Response Body (first 200 chars):\n",
      " {\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"age\": \"30\", \n",
      "    \"city\": \"New York\", \n",
      "    \"name\": \"Alice\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, defl\n",
      "\n",
      "Extracted 'json' data from response: None\n",
      "Extracted 'form' data from response: {'age': '30', 'city': 'New York', 'name': 'Alice'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of a public test page for POST requests\n",
    "url_post = \"https://httpbin.org/post\" # This service echoes back your POST request\n",
    "\n",
    "# Data to send in the POST request (e.g., form data)\n",
    "payload = {'name': 'Alice', 'age': 30, 'city': 'New York'}\n",
    "\n",
    "print(f\"\\nMaking a POST request to: {url_post} with data: {payload}\")\n",
    "try:\n",
    "    response = requests.post(url_post, data=payload) # 'data' for form-encoded data\n",
    "\n",
    "    response.raise_for_status()\n",
    "\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(\"\\nResponse Body (first 200 chars):\\n\", response.text[:200])\n",
    "\n",
    "    # If the response is JSON, you can directly parse it\n",
    "    json_response = response.json()\n",
    "    print(f\"\\nExtracted 'json' data from response: {json_response.get('json')}\")\n",
    "    print(f\"Extracted 'form' data from response: {json_response.get('form')}\")\n",
    "\n",
    "\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(f\"Something went wrong: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e5eea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `requests.get(url)`: Sends a GET request to the specified URL.\n",
    "  * `requests.post(url, data=payload)`: Sends a POST request. The `data` parameter is used for sending form-encoded data. For sending JSON data, use `json=payload_dict`.\n",
    "  * `response.status_code`: The HTTP status code (e.g., 200 OK, 404 Not Found).\n",
    "  * `response.text`: The content of the response as a Unicode string.\n",
    "  * `response.json()`: If the response contains JSON data, this method parses it into a Python dictionary.\n",
    "  * `response.raise_for_status()`: This is a convenient method to check if the request was successful. If the status code indicates an error (4xx or 5xx), it raises an `HTTPError`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de51ab",
   "metadata": {},
   "source": [
    "\n",
    "### Handling Headers, Parameters, and Cookies ‚öôÔ∏è\n",
    "\n",
    "Web requests often involve more than just the URL.\n",
    "\n",
    "  * **Headers:** HTTP headers provide additional information about the request or the response. This includes things like `User-Agent` (identifies the client), `Content-Type`, `Accept`, etc. Setting a `User-Agent` can sometimes help avoid being blocked by simple anti-scraping measures.\n",
    "  * **Parameters (`params`):** For GET requests, parameters are appended to the URL as query strings (e.g., `?key1=value1&key2=value2`). The `requests` library handles encoding them for you.\n",
    "  * **Cookies:** Small pieces of data sent by the server to the client and then sent back by the client on subsequent requests. They are used for session management, user tracking, etc. `requests` handles session cookies automatically.\n",
    "\n",
    "#### Example with Headers and Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795bfb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making a GET request with custom headers and parameters.\n",
      "Status Code: 200\n",
      "Request URL: https://httpbin.org/get?search_query=Python+web+scraping&page=1&sort=relevance\n",
      "\n",
      "Response Body (first 200 chars):\n",
      " {\n",
      "  \"args\": {\n",
      "    \"page\": \"1\", \n",
      "    \"search_query\": \"Python web scraping\", \n",
      "    \"sort\": \"relevance\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp\n",
      "\n",
      "Extracted 'headers' from response: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.88 Safari/537.36\n",
      "Extracted 'args' (parameters) from response: {'page': '1', 'search_query': 'Python web scraping', 'sort': 'relevance'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url_params = \"https://httpbin.org/get\"\n",
    "\n",
    "# Custom Headers (e.g., to mimic a browser)\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.88 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "}\n",
    "\n",
    "# Query Parameters\n",
    "params = {\n",
    "    'search_query': 'Python web scraping',\n",
    "    'page': 1,\n",
    "    'sort': 'relevance'\n",
    "}\n",
    "\n",
    "print(f\"\\nMaking a GET request with custom headers and parameters.\")\n",
    "try:\n",
    "    response = requests.get(url_params, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"Request URL: {response.url}\") # The actual URL with encoded parameters\n",
    "    print(\"\\nResponse Body (first 200 chars):\\n\", response.text[:200])\n",
    "\n",
    "    json_response = response.json()\n",
    "    print(f\"\\nExtracted 'headers' from response: {json_response.get('headers').get('User-Agent')}\")\n",
    "    print(f\"Extracted 'args' (parameters) from response: {json_response.get('args')}\")\n",
    "\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(f\"Something went wrong: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f6475",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `headers=headers`: Passes a dictionary of custom headers.\n",
    "  * `params=params`: Passes a dictionary of query parameters. `requests` automatically encodes these into the URL (e.g., `?search_query=Python+web+scraping&page=1&sort=relevance`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100e1eb",
   "metadata": {},
   "source": [
    "\n",
    "### Error Handling (Status Codes) ‚ö†Ô∏è\n",
    "\n",
    "HTTP status codes are crucial for understanding the outcome of your request.\n",
    "\n",
    "  * **2xx (Success):** The request was successfully received, understood, and accepted.\n",
    "      * `200 OK`: The most common successful response.\n",
    "  * **3xx (Redirection):** Further action needs to be taken to complete the request.\n",
    "      * `301 Moved Permanently`\n",
    "      * `302 Found`\n",
    "  * **4xx (Client Error):** The request contains bad syntax or cannot be fulfilled.\n",
    "      * `400 Bad Request`\n",
    "      * `401 Unauthorized`\n",
    "      * `403 Forbidden`: You don't have permission to access the resource (often due to anti-scraping measures).\n",
    "      * `404 Not Found`: The requested resource could not be found.\n",
    "      * `429 Too Many Requests`: You're sending too many requests in a given amount of time (rate limiting).\n",
    "  * **5xx (Server Error):** The server failed to fulfill an apparently valid request.\n",
    "      * `500 Internal Server Error`\n",
    "      * `503 Service Unavailable`: The server is currently unable to handle the request due to temporary overloading or maintenance.\n",
    "\n",
    "As shown in the examples, `response.raise_for_status()` is your friend for quick error checks. For more granular control, you can check `response.status_code` directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008dc0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Error Handling Examples ---\n",
      "\n",
      "Attempting to access: https://httpbin.org/status/200\n",
      "SUCCESS: Status Code 200 for https://httpbin.org/status/200\n",
      "\n",
      "Attempting to access: https://httpbin.org/status/404\n",
      "HTTP Error for https://httpbin.org/status/404: 404 Client Error: NOT FOUND for url: https://httpbin.org/status/404\n",
      "\n",
      "Attempting to access: https://httpbin.org/status/403\n",
      "HTTP Error for https://httpbin.org/status/403: 403 Client Error: FORBIDDEN for url: https://httpbin.org/status/403\n",
      "\n",
      "Attempting to access: https://httpbin.org/status/500\n",
      "HTTP Error for https://httpbin.org/status/500: 500 Server Error: INTERNAL SERVER ERROR for url: https://httpbin.org/status/500\n",
      "\n",
      "Attempting to access: https://nonexistent-domain-12345.com\n",
      "Connection Error for https://nonexistent-domain-12345.com: HTTPSConnectionPool(host='nonexistent-domain-12345.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000202D8E9B770>: Failed to resolve 'nonexistent-domain-12345.com' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Example of handling different status codes\n",
    "urls_to_test = [\n",
    "    \"https://httpbin.org/status/200\", # OK\n",
    "    \"https://httpbin.org/status/404\", # Not Found\n",
    "    \"https://httpbin.org/status/403\", # Forbidden\n",
    "    \"https://httpbin.org/status/500\", # Internal Server Error\n",
    "    \"https://nonexistent-domain-12345.com\" # Connection Error\n",
    "]\n",
    "\n",
    "print(\"\\n--- Error Handling Examples ---\")\n",
    "for url in urls_to_test:\n",
    "    print(f\"\\nAttempting to access: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5) # Add a timeout\n",
    "        response.raise_for_status() # Check for 4xx/5xx errors\n",
    "        print(f\"SUCCESS: Status Code {response.status_code} for {url}\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP Error for {url}: {e}\")\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"Connection Error for {url}: {e}\")\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        print(f\"Timeout Error for {url}: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An unknown error occurred for {url}: {e}\")\n",
    "    time.sleep(1) # Be polite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170b60d",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * The `try...except` block is essential for robust web scraping. It allows your script to gracefully handle network issues, server errors, or timeouts instead of crashing.\n",
    "  * `requests.exceptions.HTTPError`: Catches errors specifically related to bad HTTP status codes (4xx, 5xx).\n",
    "  * `requests.exceptions.ConnectionError`: Catches errors when your script can't connect to the server (e.g., no internet, incorrect domain).\n",
    "  * `requests.exceptions.Timeout`: Catches errors if the server doesn't respond within a specified time limit.\n",
    "  * `requests.exceptions.RequestException`: A base class for all `requests` exceptions, good for a general catch-all.\n",
    "  * `timeout=5`: It's always a good idea to set a timeout for your requests to prevent your script from hanging indefinitely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b9744",
   "metadata": {},
   "source": [
    "\n",
    "#### ‚ùì Quick Quiz: HTTP Requests with `requests`\n",
    "\n",
    "1.  Which `requests` method is typically used to retrieve data from a web server?\n",
    "\n",
    "      * A) `requests.post()`\n",
    "      * B) `requests.put()`\n",
    "      * C) `requests.get()`\n",
    "      * D) `requests.delete()`\n",
    "\n",
    "2.  If a web server responds with an HTTP status code of `403`, what does it most likely mean?\n",
    "\n",
    "      * A) The request was successful.\n",
    "      * B) The page was not found.\n",
    "      * C) You are forbidden from accessing the resource.\n",
    "      * D) The server is overloaded.\n",
    "\n",
    "3.  To send additional information like your `User-Agent` or `Accept-Language` with your request, which parameter of `requests.get()` or `requests.post()` would you use?\n",
    "\n",
    "      * A) `data`\n",
    "      * B) `params`\n",
    "      * C) `headers`\n",
    "      * D) `json`\n",
    "\n",
    "*(Answers are at the end of the section\\!)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299756fe",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "# 3.3 Beautiful Soup Fundamentals\n",
    "\n",
    "Once you've successfully fetched the raw HTML content of a webpage using `requests`, it's just a long string. Trying to extract specific pieces of information from this string using regular expressions or manual string manipulation would be a nightmare. This is where **Beautiful Soup** comes in\\! üç≤\n",
    "\n",
    "### What is Beautiful Soup? (Parsing HTML/XML) üìÑ\n",
    "\n",
    "Beautiful Soup is a Python library designed for pulling data out of HTML and XML files. It creates a parse tree from the page source, which you can navigate and search in a very Pythonic way. It gracefully handles malformed HTML, making it robust for real-world web pages.\n",
    "\n",
    "Think of it as transforming a messy, unstructured text document into a structured, easily traversable object that mirrors the hierarchy of the webpage.\n",
    "\n",
    "### Installation and Basic Usage üíª\n",
    "\n",
    "You'll need to install Beautiful Soup. It's often installed as `beautifulsoup4`:\n",
    "`pip install beautifulsoup4`\n",
    "\n",
    "You'll also need a parser. The default is usually `html.parser` (built-in to Python), but `lxml` is often recommended for its speed and robustness:\n",
    "`pip install lxml`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51398fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Basic Beautiful Soup Usage ---\n",
      "Pretty-printed HTML:\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   My Awesome Page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1 class=\"main-title\">\n",
      "   Welcome to My Site\n",
      "  </h1>\n",
      "  <p class=\"intro-paragraph\">\n",
      "   This is an\n",
      "   <b>\n",
      "    introductory\n",
      "   </b>\n",
      "   paragraph.\n",
      "  </p>\n",
      "  <div id=\"content\">\n",
      "   <ul>\n",
      "    <li>\n",
      "     Item 1\n",
      "    </li>\n",
      "    <li class=\"special\">\n",
      "     Item 2\n",
      "    </li>\n",
      "    <li>\n",
      "     Item 3\n",
      "    </li>\n",
      "   </ul>\n",
      "   <a href=\"https://example.com/about\">\n",
      "    About Us\n",
      "   </a>\n",
      "   <img alt=\"Company Logo\" src=\"/images/logo.png\"/>\n",
      "  </div>\n",
      "  <div class=\"footer\">\n",
      "   <p>\n",
      "    Copyright ¬© 2023\n",
      "   </p>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n",
      "\n",
      "\n",
      "Page Title: My Awesome Page\n",
      "First H1 Tag: <h1 class=\"main-title\">Welcome to My Site</h1>\n",
      "Text in first H1: Welcome to My Site\n",
      "First Paragraph Tag: <p class=\"intro-paragraph\">This is an <b>introductory</b> paragraph.</p>\n",
      "Text in first P: None\n",
      "All Text in first P (get_text): This is an introductory paragraph.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Example HTML content (often obtained from requests.get().text)\n",
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>My Awesome Page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1 class=\"main-title\">Welcome to My Site</h1>\n",
    "    <p class=\"intro-paragraph\">This is an <b>introductory</b> paragraph.</p>\n",
    "    <div id=\"content\">\n",
    "        <ul>\n",
    "            <li>Item 1</li>\n",
    "            <li class=\"special\">Item 2</li>\n",
    "            <li>Item 3</li>\n",
    "        </ul>\n",
    "        <a href=\"https://example.com/about\">About Us</a>\n",
    "        <img src=\"/images/logo.png\" alt=\"Company Logo\">\n",
    "    </div>\n",
    "    <div class=\"footer\">\n",
    "        <p>Copyright &copy; 2023</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Create a BeautifulSoup object\n",
    "# The first argument is the HTML string\n",
    "# The second argument is the parser to use (e.g., 'html.parser', 'lxml', 'xml')\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "print(\"--- Basic Beautiful Soup Usage ---\")\n",
    "# Pretty print the parsed HTML (makes it readable)\n",
    "print(\"Pretty-printed HTML:\\n\")\n",
    "print(soup.prettify())\n",
    "\n",
    "# Get the title of the page\n",
    "page_title = soup.title.string\n",
    "print(f\"\\nPage Title: {page_title}\")\n",
    "\n",
    "# Get the first h1 tag\n",
    "h1_tag = soup.h1\n",
    "print(f\"First H1 Tag: {h1_tag}\")\n",
    "\n",
    "# Get the text inside the first h1 tag\n",
    "h1_text = soup.h1.string\n",
    "print(f\"Text in first H1: {h1_text}\")\n",
    "\n",
    "# Get the first paragraph\n",
    "p_tag = soup.p\n",
    "print(f\"First Paragraph Tag: {p_tag}\")\n",
    "print(f\"Text in first P: {p_tag.string}\") # Note: this will return None if there are nested tags like <b>\n",
    "\n",
    "# To get all text content from a tag, including nested tags:\n",
    "p_text_all = p_tag.get_text()\n",
    "print(f\"All Text in first P (get_text): {p_text_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652b84a",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `from bs4 import BeautifulSoup`: Imports the necessary class.\n",
    "  * `BeautifulSoup(html_doc, 'html.parser')`: This is the core step. It takes your raw HTML string and parses it into a traversable `BeautifulSoup` object.\n",
    "  * `soup.prettify()`: A useful method for debugging, it formats the HTML with proper indentation, making it easier to read.\n",
    "  * `soup.tagname`: You can access a tag directly as an attribute of the `soup` object (e.g., `soup.title`, `soup.h1`). This gives you the *first* instance of that tag.\n",
    "  * `tag.string`: Accesses the direct text content within a tag. Be careful: if a tag contains other tags (like `<b>` inside `<p>`), `tag.string` might return `None`.\n",
    "  * `tag.get_text()`: A more robust way to get all the text content, including text from nested tags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15133176",
   "metadata": {},
   "source": [
    "\n",
    "### Creating a `BeautifulSoup` Object üèóÔ∏è\n",
    "\n",
    "As shown above, creating the object is simple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Step 1: Make an HTTP GET request to get the raw HTML\n",
    "url_to_scrape = \"http://books.toscrape.com/\" # A sandbox site for scraping practice\n",
    "try:\n",
    "    response = requests.get(url_to_scrape, timeout=5)\n",
    "    response.raise_for_status() # Check for HTTP errors\n",
    "    html_content = response.text\n",
    "\n",
    "    # Step 2: Create a BeautifulSoup object from the HTML content\n",
    "    soup_object = BeautifulSoup(html_content, 'lxml') # Using 'lxml' for potentially faster parsing\n",
    "\n",
    "    print(f\"\\nBeautifulSoup object created from {url_to_scrape} using 'lxml' parser.\")\n",
    "    print(f\"Type of soup_object: {type(soup_object)}\")\n",
    "\n",
    "    # You can now proceed to navigate and extract data from soup_object\n",
    "    print(f\"Title of the scraped page: {soup_object.title.string}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Failed to retrieve content from {url_to_scrape}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd9510",
   "metadata": {},
   "source": [
    "\n",
    "**Key parsers:**\n",
    "\n",
    "  * `html.parser`: Built-in, decent speed, good for most cases.\n",
    "  * `lxml`: Very fast, robust, handles malformed HTML well. Requires `pip install lxml`.\n",
    "  * `html5lib`: Extremely tolerant of malformed HTML, parses like a web browser. Slower. Requires `pip install html5lib`.\n",
    "\n",
    "For general-purpose scraping, `lxml` is a popular choice for its balance of speed and robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcea4d1",
   "metadata": {},
   "source": [
    "\n",
    "### Navigating the Parse Tree (Tags, Names, Attributes, Strings) üå≥\n",
    "\n",
    "Beautiful Soup represents the HTML document as a tree structure. You can navigate this tree using various properties and methods.\n",
    "\n",
    "  * **Tags:** HTML elements are represented as `Tag` objects.\n",
    "      * `soup.tag_name`: Accesses the *first* tag with that name.\n",
    "      * `tag.name`: Gets the name of the tag (e.g., `'a'`, `'div'`, `'p'`).\n",
    "      * `tag['attribute_name']`: Accesses the value of an attribute (e.g., `a_tag['href']`).\n",
    "      * `tag.attrs`: A dictionary of all attributes of a tag.\n",
    "  * **Strings:** The text content within a tag.\n",
    "      * `tag.string`: Direct string content (careful with nested tags).\n",
    "      * `tag.get_text()`: All text content, including nested tags.\n",
    "  * **Navigation:**\n",
    "      * `contents`: A list of the tag's direct children.\n",
    "      * `children`: A generator that yields the tag's direct children.\n",
    "      * `parent`: The parent tag.\n",
    "      * `next_sibling`, `previous_sibling`: Next/previous tags at the same level.\n",
    "\n",
    "<!-- end list -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc_nav = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>Navigation Example</title>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"header\">\n",
    "        <h1 class=\"main-heading\">My Blog</h1>\n",
    "        <p>A place for thoughts.</p>\n",
    "    </div>\n",
    "    <div id=\"posts\">\n",
    "        <div class=\"post\">\n",
    "            <h2>Post Title 1</h2>\n",
    "            <p>Content of post 1.</p>\n",
    "            <a href=\"/post1\">Read More</a>\n",
    "        </div>\n",
    "        <div class=\"post\">\n",
    "            <h2>Post Title 2</h2>\n",
    "            <p>Content of post 2.</p>\n",
    "            <a href=\"/post2\">Read More</a>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div class=\"footer\">\n",
    "        <p>Contact: <a href=\"mailto:info@example.com\">info@example.com</a></p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup_nav = BeautifulSoup(html_doc_nav, 'html.parser')\n",
    "\n",
    "print(\"\\n--- Navigating the Parse Tree ---\")\n",
    "\n",
    "# Accessing a specific tag\n",
    "header_div = soup_nav.find(id=\"header\") # More robust way to find by ID\n",
    "print(f\"Header Div Tag: {header_div}\")\n",
    "\n",
    "# Accessing a child tag\n",
    "main_heading = header_div.h1\n",
    "print(f\"\\nMain Heading (child of header_div): {main_heading}\")\n",
    "print(f\"Main Heading Class Attribute: {main_heading['class']}\")\n",
    "\n",
    "# Accessing attributes\n",
    "read_more_link = soup_nav.a # Gets the first <a> tag\n",
    "print(f\"\\nFirst Read More Link: {read_more_link}\")\n",
    "print(f\"Href attribute of link: {read_more_link['href']}\")\n",
    "print(f\"All attributes of link: {read_more_link.attrs}\")\n",
    "\n",
    "# Getting text content\n",
    "post_div = soup_nav.find(class_=\"post\") # Gets the first div with class=\"post\"\n",
    "print(f\"\\nFirst Post Div:\\n{post_div.prettify()}\")\n",
    "# Get text from its h2 child\n",
    "post_h2_text = post_div.h2.get_text()\n",
    "print(f\"Text of H2 in first post: {post_h2_text}\")\n",
    "\n",
    "# Navigating siblings\n",
    "first_p_in_header = header_div.p\n",
    "print(f\"\\nFirst P in header: {first_p_in_header}\")\n",
    "# next_sibling might return a newline character or whitespace, so often combine with find_next_sibling\n",
    "next_to_p = first_p_in_header.find_next_sibling()\n",
    "print(f\"Next sibling to first P in header: {next_to_p}\") # Should be None in this example as it's the last child\n",
    "\n",
    "# Accessing all children vs. direct content\n",
    "ul_tag = soup_nav.ul\n",
    "print(f\"\\nUL Tag:\\n{ul_tag.prettify()}\")\n",
    "print(f\"UL Tag's contents (direct children including NavigableString):\\n{ul_tag.contents}\")\n",
    "print(f\"UL Tag's children (generator, typically used in loops):\\n{[child for child in ul_tag.children]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad32014",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `soup.find(id=\"...\")` or `soup.find(class_=\"...\")`: More powerful ways to locate specific tags than direct attribute access, especially when you need to search beyond the first occurrence or by attributes. We'll explore `find()` and `find_all()` more deeply in the next section.\n",
    "  * `tag['attribute_name']`: Accesses the value of an HTML attribute (e.g., `href`, `class`, `id`).\n",
    "  * `tag.attrs`: Returns a dictionary of all attributes of a tag.\n",
    "  * `tag.contents`: Returns a list of the tag's direct children (which can be other tags or `NavigableString` objects representing text).\n",
    "  * `tag.children`: Returns a *generator* that yields the tag's direct children. This is more memory-efficient for many children.\n",
    "  * `tag.parent`: Returns the parent tag.\n",
    "  * `tag.next_sibling`, `tag.previous_sibling`: These return the next/previous sibling in the parse tree. Be aware that whitespace (like newlines between tags in the HTML) can also be siblings, represented as `NavigableString` objects.\n",
    "\n",
    "**Key takeaway:** Beautiful Soup transforms the HTML into a navigable Python object, allowing you to move up, down, and sideways through the document's structure to pinpoint the data you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f1bd0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### ‚ùì Quick Quiz: Beautiful Soup Fundamentals\n",
    "\n",
    "1.  Which of the following is *not* a common parser used with Beautiful Soup?\n",
    "\n",
    "      * A) `html.parser`\n",
    "      * B) `json.parser`\n",
    "      * C) `lxml`\n",
    "      * D) `html5lib`\n",
    "\n",
    "2.  If you have a Beautiful Soup `tag` object and want to get all the text content within it, including text from any nested tags, which method should you use?\n",
    "\n",
    "      * A) `tag.string`\n",
    "      * B) `tag.text()`\n",
    "      * C) `tag.get_text()`\n",
    "      * D) `tag.content()`\n",
    "\n",
    "3.  To access the value of an attribute named `id` from a `Tag` object called `my_tag`, what is the correct syntax?\n",
    "\n",
    "      * A) `my_tag.id`\n",
    "      * B) `my_tag.attribute('id')`\n",
    "      * C) `my_tag['id']`\n",
    "      * D) `my_tag.get_id()`\n",
    "\n",
    "4.  True or False: `tag.contents` will return a list of all descendants of a tag, not just direct children.\n",
    "\n",
    "*(Answers are at the end of the section\\!)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9d48f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### Quick Quiz Answers:\n",
    "\n",
    "**Web Scraping Introduction & Ethics:**\n",
    "\n",
    "1.  **C) `robots.txt`**\n",
    "2.  **C) Introducing random delays between requests.**\n",
    "3.  **False** (Even without `robots.txt`, Terms of Service, legal implications, and server load considerations still apply.)\n",
    "\n",
    "**HTTP Requests with `requests`:**\n",
    "\n",
    "1.  **C) `requests.get()`**\n",
    "2.  **C) You are forbidden from accessing the resource.** (`403 Forbidden`)\n",
    "3.  **C) `headers`**\n",
    "\n",
    "**Beautiful Soup Fundamentals:**\n",
    "\n",
    "1.  **B) `json.parser`** (Beautiful Soup is for HTML/XML, not JSON directly)\n",
    "2.  **C) `tag.get_text()`**\n",
    "3.  **C) `my_tag['id']`**\n",
    "4.  **False** (`tag.contents` returns only direct children. For all descendants, you might iterate or use methods like `find_all`.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293dd94",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "# 3.4 Searching the Parse Tree\n",
    "\n",
    "This is where Beautiful Soup truly shines\\! Instead of sifting through raw text, you can precisely locate elements based on their tag name, attributes (like `class` or `id`), or even their text content.\n",
    "\n",
    "### üìö Table of Contents:\n",
    "\n",
    "  * **3.4 Searching the Parse Tree** üïµÔ∏è‚Äç‚ôÄÔ∏è\n",
    "      * `find()` and `find_all()` methods üîç\n",
    "      * Searching by Tag Name, Attributes, and Text Content üè∑Ô∏è\n",
    "      * Using CSS Selectors (`select()`, `select_one()`) üéØ\n",
    "      * Regular Expressions in Searches üß©\n",
    "  * **3.5 Modifying the Parse Tree** ‚úçÔ∏è\n",
    "      * Adding, Removing, and Modifying Tags and Attributes ‚ûï‚ûñ\n",
    "      * Inserting Content üìù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d4794",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "### `find()` and `find_all()` methods üîç\n",
    "\n",
    "These are your primary tools for navigating and searching the parse tree.\n",
    "\n",
    "  * **`find(name, attrs, recursive, string, **kwargs)`**:\n",
    "      * Returns the *first* matching tag.\n",
    "      * If no match is found, it returns `None`.\n",
    "  * **`find_all(name, attrs, recursive, string, limit, **kwargs)`**:\n",
    "      * Returns a *list* of all matching tags.\n",
    "      * If no matches are found, it returns an empty list `[]`.\n",
    "      * `limit`: Stops searching after finding a specified number of matches (useful for performance).\n",
    "\n",
    "Let's use our sample HTML for demonstration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Awesome Products</title>\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1 id=\"main-header\" class=\"site-title\">Our Store</h1>\n",
    "        <p>Your one-stop shop for amazing items.</p>\n",
    "    </header>\n",
    "    <div class=\"product-list\">\n",
    "        <div class=\"product\" data-id=\"101\">\n",
    "            <h2>Product A</h2>\n",
    "            <p class=\"price\">$19.99</p>\n",
    "            <span class=\"stock out-of-stock\">Out of Stock</span>\n",
    "        </div>\n",
    "        <div class=\"product\" data-id=\"102\">\n",
    "            <h2>Product B</h2>\n",
    "            <p class=\"price\">$29.50</p>\n",
    "            <span class=\"stock in-stock\">In Stock</span>\n",
    "        </div>\n",
    "        <div class=\"product\" data-id=\"103\">\n",
    "            <h2>Product C</h2>\n",
    "            <p class=\"price\">$5.00</p>\n",
    "            <span class=\"stock in-stock\">In Stock</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    <footer>\n",
    "        <p class=\"copyright\">¬© 2023 All rights reserved.</p>\n",
    "        <a href=\"/contact\">Contact Us</a>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_doc, 'lxml')\n",
    "\n",
    "print(\"--- Using find() and find_all() ---\")\n",
    "\n",
    "# find(): Get the first <h2> tag\n",
    "first_h2 = soup.find('h2')\n",
    "print(f\"First <h2> tag: {first_h2}\")\n",
    "\n",
    "# find_all(): Get all <p> tags\n",
    "all_p_tags = soup.find_all('p')\n",
    "print(f\"\\nAll <p> tags found ({len(all_p_tags)} total):\")\n",
    "for p in all_p_tags:\n",
    "    print(f\" - {p}\")\n",
    "\n",
    "# find_all() with limit\n",
    "first_two_products = soup.find_all('div', class_='product', limit=2)\n",
    "print(f\"\\nFirst two products found ({len(first_two_products)} total):\")\n",
    "for prod in first_two_products:\n",
    "    print(f\" - {prod['data-id']}\") # Accessing attribute\n",
    "\n",
    "# find() will return None if not found\n",
    "non_existent_tag = soup.find('xyz')\n",
    "print(f\"\\nSearching for non-existent tag 'xyz': {non_existent_tag}\")\n",
    "\n",
    "# find_all() will return an empty list if not found\n",
    "non_existent_tags = soup.find_all('xyz')\n",
    "print(f\"Searching for non-existent tags 'xyz': {non_existent_tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088d932",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `soup.find('h2')`: Finds the first `<h2>` tag.\n",
    "  * `soup.find_all('p')`: Finds all `<p>` tags and returns them in a list.\n",
    "  * `limit`: In `find_all`, this is very useful when you only need a few results (e.g., the first 10 products) and don't want to parse the entire document if it's very large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c6eee",
   "metadata": {},
   "source": [
    "\n",
    "### Searching by Tag Name, Attributes, and Text Content üè∑Ô∏è\n",
    "\n",
    "You can combine arguments to make your searches very specific.\n",
    "\n",
    "#### By Tag Name\n",
    "\n",
    "Already demonstrated above: `soup.find('div')`, `soup.find_all('li')`.\n",
    "\n",
    "#### By Attributes\n",
    "\n",
    "This is crucial for targeting specific elements that have unique `id`s or common `class`es.\n",
    "\n",
    "  * `id`: Use the `id` keyword argument directly.\n",
    "  * `class`: Use the `class_` keyword argument (because `class` is a reserved keyword in Python). You can pass a string or a list of strings for multiple classes.\n",
    "  * Other attributes: Pass them as keyword arguments directly.\n",
    "\n",
    "<!-- end list -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037fec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Searching by Attributes ---\")\n",
    "\n",
    "# Search by ID: Get the main header\n",
    "main_header = soup.find(id='main-header')\n",
    "print(f\"Element with id 'main-header': {main_header.get_text()}\")\n",
    "\n",
    "# Search by class: Get all elements with class 'product'\n",
    "all_products = soup.find_all(class_='product')\n",
    "print(f\"\\nAll elements with class 'product' ({len(all_products)} total):\")\n",
    "for product in all_products:\n",
    "    print(f\" - {product.h2.get_text()} (ID: {product['data-id']})\")\n",
    "\n",
    "# Search by multiple classes: Find stock status that is both 'stock' and 'out-of-stock'\n",
    "out_of_stock_span = soup.find(class_=['stock', 'out-of-stock'])\n",
    "print(f\"\\nOut of Stock Status: {out_of_stock_span.get_text()}\")\n",
    "\n",
    "# Search by a custom attribute (data-id)\n",
    "product_102 = soup.find(attrs={'data-id': '102'})\n",
    "print(f\"\\nProduct with data-id '102': {product_102.h2.get_text()}\")\n",
    "# Or directly as a keyword argument (if attribute name is a valid Python identifier)\n",
    "# product_102_alt = soup.find(data_id='102') # this often works, but attrs dict is more robust for tricky names\n",
    "# print(f\"Product with data-id '102' (alt method): {product_102_alt.h2.get_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f657bb7",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `id='main-header'`: Direct keyword argument for `id`.\n",
    "  * `class_='product'`: Note the underscore `_` for the `class` attribute.\n",
    "  * `class_=['stock', 'out-of-stock']`: Can search for elements that have *all* specified classes (if they appear in the same order as in the list, or any order if they are just present).\n",
    "  * `attrs={'data-id': '102'}`: Use a dictionary for attributes, especially useful for attributes with hyphens (`-`) or other characters that aren't valid Python identifier names.\n",
    "\n",
    "#### By Text Content (`string` argument)\n",
    "\n",
    "You can also search for tags based on the text they contain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Searching by Text Content ---\")\n",
    "\n",
    "# Find a paragraph containing \"amazing\"\n",
    "paragraph_with_amazing = soup.find('p', string=\"Your one-stop shop for amazing items.\")\n",
    "print(f\"Paragraph with exact text 'Your one-stop shop for amazing items.': {paragraph_with_amazing.get_text()}\")\n",
    "\n",
    "# Find any tag whose string is exactly \"In Stock\"\n",
    "in_stock_span = soup.find('span', string=\"In Stock\")\n",
    "print(f\"First 'In Stock' span: {in_stock_span.get_text()}\")\n",
    "\n",
    "# Find all 'In Stock' spans\n",
    "all_in_stock = soup.find_all('span', string=\"In Stock\")\n",
    "print(f\"All 'In Stock' spans ({len(all_in_stock)} total):\")\n",
    "for s in all_in_stock:\n",
    "    print(f\" - {s.get_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ccae9f",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `string=\"Exact Text\"`: Matches tags whose direct string content is *exactly* the specified text. Be precise\\!\n",
    "\n",
    "### Using CSS Selectors (`select()`, `select_one()`) üéØ\n",
    "\n",
    "If you're familiar with CSS, Beautiful Soup allows you to use CSS selectors to find elements. This can be very powerful and concise for complex selections.\n",
    "\n",
    "  * **`select_one(selector)`**: Returns the *first* element matching the CSS selector.\n",
    "  * **`select(selector)`**: Returns a *list* of all elements matching the CSS selector.\n",
    "\n",
    "<!-- end list -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Using CSS Selectors (select() and select_one()) ---\")\n",
    "\n",
    "# Select the main header by ID: #id_name\n",
    "main_header_css = soup.select_one('#main-header')\n",
    "print(f\"Main header using CSS selector '#main-header': {main_header_css.get_text()}\")\n",
    "\n",
    "# Select all product prices by class: .class_name\n",
    "all_prices_css = soup.select('.price')\n",
    "print(f\"\\nAll prices using CSS selector '.price' ({len(all_prices_css)} total):\")\n",
    "for price_tag in all_prices_css:\n",
    "    print(f\" - {price_tag.get_text()}\")\n",
    "\n",
    "# Select elements nested within others: div.product > h2\n",
    "# Find all <h2> tags that are direct children of a <div class=\"product\">\n",
    "product_h2s_css = soup.select('div.product > h2')\n",
    "print(f\"\\nAll H2s that are direct children of .product div ({len(product_h2s_css)} total):\")\n",
    "for h2 in product_h2s_css:\n",
    "    print(f\" - {h2.get_text()}\")\n",
    "\n",
    "# Select elements by attribute value: [attr=\"value\"]\n",
    "# Find all divs with data-id attribute\n",
    "elements_with_data_id = soup.select('[data-id]')\n",
    "print(f\"\\nElements with 'data-id' attribute ({len(elements_with_data_id)} total):\")\n",
    "for elem in elements_with_data_id:\n",
    "    print(f\" - {elem['data-id']}\")\n",
    "\n",
    "# Select elements based on partial attribute match (starts with, ends with, contains)\n",
    "# [attr^=\"prefix\"] - starts with\n",
    "# [attr$=\"suffix\"] - ends with\n",
    "# [attr*=\"substring\"] - contains\n",
    "contact_link_css = soup.select_one('a[href=\"/contact\"]') # exact match\n",
    "print(f\"\\nContact link using CSS selector 'a[href=\\\"/contact\\\"]': {contact_link_css['href']}\")\n",
    "\n",
    "# Select a combination: div.product span.in-stock\n",
    "# Find all <span> tags with class 'in-stock' that are descendants of <div class=\"product\">\n",
    "in_stock_spans_css = soup.select('div.product span.in-stock')\n",
    "print(f\"\\nIn stock status using CSS selector 'div.product span.in-stock' ({len(in_stock_spans_css)} total):\")\n",
    "for span in in_stock_spans_css:\n",
    "    print(f\" - {span.get_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a819912",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * **`#id_name`**: Selects by ID.\n",
    "  * **`.class_name`**: Selects by class.\n",
    "  * **`tag_name.class_name`**: Selects a tag with a specific class.\n",
    "  * **`parent_tag > child_tag`**: Selects direct children.\n",
    "  * **`ancestor descendant`**: Selects descendants (any level).\n",
    "  * **`[attribute_name]`**: Selects elements that have the attribute.\n",
    "  * **`[attribute_name=\"value\"]`**: Selects elements where the attribute has an exact value.\n",
    "  * **`[attr^=\"prefix\"]`, `[attr$=\"suffix\"]`, `[attr*=\"substring\"]`**: Powerful for partial attribute matches.\n",
    "\n",
    "**Pro Tip\\!** üí° If you know CSS selectors well, `select()` and `select_one()` can often be more concise and expressive than complex `find_all()` calls, especially for nested or attribute-based selections.\n",
    "\n",
    "### Regular Expressions in Searches üß©\n",
    "\n",
    "Beautiful Soup allows you to pass a regular expression object to the `name` (tag name), `attrs` (attribute values), or `string` arguments of `find()` and `find_all()`. This is incredibly flexible for pattern-based matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c98122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc_re = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <div id=\"item-product-123\">Product A</div>\n",
    "    <div id=\"item-product-456\">Product B</div>\n",
    "    <p>Some random text</p>\n",
    "    <a href=\"/category/electronics\">Electronics</a>\n",
    "    <a href=\"/category/books\">Books</a>\n",
    "    <span class=\"status-active\">Active</span>\n",
    "    <span class=\"status-inactive\">Inactive</span>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup_re = BeautifulSoup(html_doc_re, 'lxml')\n",
    "\n",
    "print(\"\\n--- Using Regular Expressions in Searches ---\")\n",
    "\n",
    "# Search for tag names starting with 'h'\n",
    "h_tags = soup_re.find_all(re.compile(\"^h\")) # Matches h1, h2, head, html (if they exist)\n",
    "print(f\"Tags starting with 'h': {[tag.name for tag in h_tags]}\")\n",
    "\n",
    "# Search for `id` attributes that contain 'product'\n",
    "product_divs = soup_re.find_all('div', id=re.compile(\"product\"))\n",
    "print(f\"\\nDivs with 'product' in their ID: {[div['id'] for div in product_divs]}\")\n",
    "\n",
    "# Search for `href` attributes starting with '/category/'\n",
    "category_links = soup_re.find_all('a', href=re.compile(\"^/category/\"))\n",
    "print(f\"\\nLinks starting with '/category/': {[link['href'] for link in category_links]}\")\n",
    "\n",
    "# Search for class names containing 'status-'\n",
    "status_spans = soup_re.find_all('span', class_=re.compile(\"status-\"))\n",
    "print(f\"\\nSpans with class containing 'status-': {[span['class'] for span in status_spans]}\")\n",
    "\n",
    "# Search for string content that contains \"random\"\n",
    "random_text_p = soup_re.find('p', string=re.compile(\"random\"))\n",
    "print(f\"\\nParagraph containing 'random': {random_text_p.get_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b6c98",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `re.compile(\"pattern\")`: Compiles a regular expression.\n",
    "  * `name=re.compile(\"^h\")`: Matches tag names that *start* with 'h'.\n",
    "  * `id=re.compile(\"product\")`: Matches `id` attribute values that *contain* 'product'.\n",
    "  * `class_=re.compile(\"status-\")`: Matches `class` attribute values that *contain* 'status-'.\n",
    "  * `string=re.compile(\"random\")`: Matches string content that *contains* 'random'.\n",
    "\n",
    "**Common Pitfall\\!** ‚ö†Ô∏è When searching by `string` with a `re.compile` object, ensure the regex matches the *entire* string content of the tag, or use `re.search` for partial matches if the `string` argument itself doesn't offer enough flexibility (though `re.compile` within `string` often implicitly searches for substring). When using `string=re.compile(...)`, it means that the direct text content of the tag (if it's just text, no nested tags) must match the regex. For text spread across nested tags, `get_text(strip=True)` and then `re.search` on that result is more robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c0b90",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "### Quick Quiz Answers: Searching the Parse Tree\n",
    "\n",
    "1.  To get a list of all `<div>` tags with the class `item`, which of the following is the most appropriate `BeautifulSoup` method call?\n",
    "\n",
    "      * A) `soup.find('div', class_='item')`\n",
    "      * B) `soup.find_all('div', id='item')`\n",
    "      * C) `soup.select('div.item')`\n",
    "      * D) `soup.select_one('div.item')`\n",
    "\n",
    "    *Correct Answer: C) `soup.select('div.item')`* (A is wrong because `find` only gets the first, B is wrong because it searches by `id` not `class`.)\n",
    "\n",
    "2.  If you want to find an `<a>` tag whose `href` attribute *starts with* \"https://secure.\", which argument and value type would you use in `find()` or `find_all()`?\n",
    "\n",
    "      * A) `href=\"https://secure.*\"`\n",
    "      * B) `href=re.compile(\"^https://secure.\")`\n",
    "      * C) `attrs={'href': 'https://secure%'}`\n",
    "      * D) `string=\"https://secure.\"`\n",
    "\n",
    "    *Correct Answer: B) `href=re.compile(\"^https://secure.\")`* (Regular expressions are needed for pattern matching on attributes.)\n",
    "\n",
    "3.  What is the main difference in the return value between `soup.find()` and `soup.find_all()` if multiple matches exist?\n",
    "\n",
    "      * A) `find()` returns `None`, `find_all()` returns the first match.\n",
    "      * B) `find()` returns a single `Tag` object, `find_all()` returns a list of `Tag` objects.\n",
    "      * C) `find()` returns a list, `find_all()` returns a dictionary.\n",
    "      * D) They both return lists, but `find()`'s list has a limit of 1.\n",
    "\n",
    "    *Correct Answer: B) `find()` returns a single `Tag` object, `find_all()` returns a list of `Tag` objects.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40127e1",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "# 3.5 Modifying the Parse Tree\n",
    "\n",
    "While web scraping primarily focuses on *extracting* data, Beautiful Soup also allows you to *modify* the parse tree. This is less common for simple data extraction but can be useful for:\n",
    "\n",
    "  * **Cleaning HTML:** Removing unwanted scripts, styles, or irrelevant sections before processing.\n",
    "  * **Preprocessing:** Adding/modifying attributes or tags to make subsequent scraping easier.\n",
    "  * **Generating new HTML:** Creating custom HTML content from scraped data.\n",
    "\n",
    "Remember, these modifications only exist within your Python script's `BeautifulSoup` object; they do not change the actual website.\n",
    "\n",
    "### Adding, Removing, and Modifying Tags and Attributes ‚ûï‚ûñ\n",
    "\n",
    "You can treat `Tag` objects much like Python dictionaries for their attributes, and lists for their children.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc_mod = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <div class=\"product\">\n",
    "        <h2 class=\"title\">Laptop XYZ</h2>\n",
    "        <p class=\"description\">Powerful and sleek.</p>\n",
    "        <p class=\"price\">$1200</p>\n",
    "    </div>\n",
    "    <div class=\"ad\">\n",
    "        <script>alert('unwanted ad script');</script>\n",
    "        <img src=\"/ads/banner.gif\">\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup_mod = BeautifulSoup(html_doc_mod, 'lxml')\n",
    "\n",
    "print(\"--- Modifying the Parse Tree ---\")\n",
    "print(\"Original HTML:\\n\", soup_mod.prettify())\n",
    "\n",
    "# 1. Modifying a tag's string content\n",
    "price_tag = soup_mod.find('p', class_='price')\n",
    "if price_tag:\n",
    "    old_price = price_tag.string\n",
    "    price_tag.string = \"$1150 (Limited Time Offer!)\" # Modify the string directly\n",
    "    print(f\"\\nModified price from '{old_price}' to '{price_tag.string}'\")\n",
    "\n",
    "# 2. Modifying an attribute\n",
    "product_div = soup_mod.find('div', class_='product')\n",
    "if product_div:\n",
    "    product_div['data-new-attr'] = 'value123' # Add a new attribute\n",
    "    product_div['class'] = 'product-updated' # Change an existing attribute\n",
    "    print(f\"\\nModified product div: {product_div.attrs}\")\n",
    "\n",
    "# 3. Removing a tag (and its contents)\n",
    "ad_div = soup_mod.find('div', class_='ad')\n",
    "if ad_div:\n",
    "    ad_div.extract() # Removes the tag and its contents from the tree\n",
    "    print(\"\\nRemoved the 'ad' div.\")\n",
    "\n",
    "# 4. Removing an attribute\n",
    "title_tag = soup_mod.find('h2', class_='title')\n",
    "if title_tag:\n",
    "    del title_tag['class'] # Delete the 'class' attribute\n",
    "    print(f\"\\nRemoved 'class' attribute from title: {title_tag}\")\n",
    "\n",
    "print(\"\\n--- Modified HTML ---\")\n",
    "print(soup_mod.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177449e",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `tag.string = \"New content\"`: Directly changes the text content of a tag. This works best when the tag *only* contains text (no nested tags).\n",
    "  * `tag['attribute_name'] = 'new_value'`: Sets or modifies an attribute's value. If the attribute doesn't exist, it's added.\n",
    "  * `del tag['attribute_name']`: Deletes an attribute.\n",
    "  * `tag.extract()`: Removes the tag *and all its children* from the parse tree. It also returns the removed tag, so you could potentially reinsert it elsewhere.\n",
    "  * `tag.decompose()`: Similar to `extract()`, but returns `None` and doesn't keep the removed tag in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304dd22",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Inserting Content üìù\n",
    "\n",
    "You can also add new tags or text content to your parse tree.\n",
    "\n",
    "  * `append()`: Adds a child to the end of a tag's `.contents`.\n",
    "  * `extend()`: Appends a list of children to the end of a tag's `.contents`.\n",
    "  * `insert(position, new_element)`: Inserts a new element at a specific position among children.\n",
    "  * `insert_before(new_element)` / `insert_after(new_element)`: Inserts an element as a sibling.\n",
    "  * `new_tag()`: Creates a new `Tag` object.\n",
    "  * `new_string()`: Creates a new `NavigableString` object.\n",
    "\n",
    "<!-- end list -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb79fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "html_doc_insert = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <div id=\"container\">\n",
    "        <h1>Header</h1>\n",
    "        <p>Paragraph 1</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup_insert = BeautifulSoup(html_doc_insert, 'lxml')\n",
    "container = soup_insert.find(id=\"container\")\n",
    "\n",
    "print(\"\\n--- Inserting Content ---\")\n",
    "print(\"Original HTML:\\n\", soup_insert.prettify())\n",
    "\n",
    "# 1. Append a new paragraph\n",
    "new_p = soup_insert.new_tag(\"p\")\n",
    "new_p.string = \"This is a new appended paragraph.\"\n",
    "container.append(new_p)\n",
    "print(\"\\nAfter appending a new paragraph:\")\n",
    "print(soup_insert.prettify())\n",
    "\n",
    "# 2. Insert a comment before the h1\n",
    "comment = NavigableString(\"\")\n",
    "h1_tag = container.h1\n",
    "h1_tag.insert_before(comment)\n",
    "print(\"\\nAfter inserting a comment before h1:\")\n",
    "print(soup_insert.prettify())\n",
    "\n",
    "# 3. Insert a new div at a specific position (e.g., after h1)\n",
    "new_div = soup_insert.new_tag(\"div\")\n",
    "new_div['class'] = 'info'\n",
    "new_div.string = \"Important information here.\"\n",
    "h1_tag.insert_after(new_div)\n",
    "print(\"\\nAfter inserting a new div after h1:\")\n",
    "print(soup_insert.prettify())\n",
    "\n",
    "# 4. Adding nested content\n",
    "another_div = soup_insert.new_tag(\"div\")\n",
    "another_div['id'] = 'nested-section'\n",
    "nested_span = soup_insert.new_tag(\"span\")\n",
    "nested_span.string = \"Hello from nested span.\"\n",
    "another_div.append(nested_span)\n",
    "container.append(another_div) # Append the div with its nested span\n",
    "print(\"\\nAfter adding nested content:\")\n",
    "print(soup_insert.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f30c2d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `soup_insert.new_tag(\"p\")`: Creates an empty `p` tag object.\n",
    "  * `soup_insert.new_string(\"text\")`: Creates a string object that can be inserted into the tree.\n",
    "  * `tag.append(child_tag_or_string)`: Adds the `child_tag_or_string` as the last child.\n",
    "  * `tag.insert_before(sibling_tag_or_string)`: Inserts the element as a sibling *before* the current tag.\n",
    "  * `tag.insert_after(sibling_tag_or_string)`: Inserts the element as a sibling *after* the current tag.\n",
    "\n",
    "**Key takeaway:** While less frequent for basic scraping, the ability to modify the parse tree gives you complete control over the HTML representation within your script, allowing for complex data manipulation or HTML generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10abc6b5",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "### Quick Quiz Answers: Modifying the Parse Tree\n",
    "\n",
    "1.  If `my_tag` is a Beautiful Soup `Tag` object, and you want to remove its `class` attribute, which of the following is the correct way?\n",
    "\n",
    "      * A) `my_tag.remove_attribute('class')`\n",
    "      * B) `my_tag['class'] = None`\n",
    "      * C) `del my_tag['class']`\n",
    "      * D) `my_tag.delete_attr('class')`\n",
    "\n",
    "    *Correct Answer: C) `del my_tag['class']`*\n",
    "\n",
    "2.  You have a `BeautifulSoup` object `soup` and you want to create a new `<div>` tag with the `id` \"new-section\" and append it as the last child of an existing tag named `parent_tag`. Which sequence of operations is correct?\n",
    "\n",
    "      * A) `new_div = soup.new_tag(\"div\"); new_div['id'] = \"new-section\"; parent_tag.append(new_div)`\n",
    "      * B) `new_div = Tag(\"div\"); new_div.id = \"new-section\"; parent_tag.add(new_div)`\n",
    "      * C) `parent_tag.append(\"<div id='new-section'></div>\")`\n",
    "      * D) `soup.add_tag(\"div\", id=\"new-section\", parent=parent_tag)`\n",
    "\n",
    "    *Correct Answer: A) `new_div = soup.new_tag(\"div\"); new_div['id'] = \"new-section\"; parent_tag.append(new_div)`*\n",
    "\n",
    "3.  What is the primary effect of calling `tag.extract()` on a Beautiful Soup `Tag` object?\n",
    "\n",
    "      * A) It changes the tag's name.\n",
    "      * B) It removes the tag and all its contents from the parse tree.\n",
    "      * C) It saves the tag to an external file.\n",
    "      * D) It converts the tag to a plain string.\n",
    "\n",
    "    *Correct Answer: B) It removes the tag and all its contents from the parse tree.*\n",
    "\n",
    "-----\n",
    "\n",
    "That wraps up the crucial aspects of **Searching and Modifying the Parse Tree** with Beautiful Soup\\! You now have a robust set of tools for pinpointing exactly what you need in an HTML document and even making changes to its structure within your script.\n",
    "\n",
    "Next, we'll move into more practical web scraping patterns and handling common challenges\\! Keep up the great work\\! ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68376872",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "\n",
    "# 3.6 Practical Beautiful Soup Applications\n",
    "\n",
    "Now, let's put our `requests` and Beautiful Soup skills to work on common web scraping tasks. We'll use a sample HTML structure that mimics real-world scenarios.\n",
    "\n",
    "### üìö Table of Contents: Web Scraping with Beautiful Soup\n",
    "\n",
    "  * **3.6 Practical Beautiful Soup Applications** üìä\n",
    "      * Extracting Data from Tables üìà\n",
    "      * Scraping Links and Images üîó\n",
    "      * Handling Nested Structures üå≥\n",
    "      * Saving Scraped Data (CSV, JSON) üíæ\n",
    "  * **3.7 Advanced Beautiful Soup Techniques** üöÄ\n",
    "      * Dealing with Common Scraping Challenges (Dynamic Content, Anti-Scraping Measures - *briefly mention*) üöß\n",
    "      * Using `lxml` Parser for Speed ‚ö°\n",
    "      * Integrating with Other Libraries (e.g., `pandas` for Data Analysis) ü§ù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576149ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Sample HTML for demonstration purposes\n",
    "# In a real scenario, you'd get this from requests.get(url).text\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Product Catalog</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Our Best Selling Products</h1>\n",
    "\n",
    "    <table id=\"product-table\">\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Product Name</th>\n",
    "                <th>Category</th>\n",
    "                <th>Price</th>\n",
    "                <th>Availability</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>Smartphone X</td>\n",
    "                <td>Electronics</td>\n",
    "                <td>$799.99</td>\n",
    "                <td>In Stock</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Laptop Pro</td>\n",
    "                <td>Electronics</td>\n",
    "                <td>$1299.00</td>\n",
    "                <td>Low Stock</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Wireless Headphones</td>\n",
    "                <td>Audio</td>\n",
    "                <td>$149.50</td>\n",
    "                <td>In Stock</td>\n",
    "            </tr>\n",
    "             <tr>\n",
    "                <td>Smartwatch S</td>\n",
    "                <td>Wearables</td>\n",
    "                <td>$299.00</td>\n",
    "                <td>Out of Stock</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    "    <h2>Latest Articles</h2>\n",
    "    <div class=\"articles\">\n",
    "        <div class=\"article-item\">\n",
    "            <h3><a href=\"/articles/article-1.html\">The Future of AI</a></h3>\n",
    "            <p>Exploring recent advancements...</p>\n",
    "            <img src=\"/images/ai_thumb.jpg\" alt=\"AI Thumbnail\">\n",
    "        </div>\n",
    "        <div class=\"article-item\">\n",
    "            <h3><a href=\"/articles/article-2.html\">Understanding Quantum Computing</a></h3>\n",
    "            <p>A beginner's guide to the basics...</p>\n",
    "            <img src=\"/images/qc_thumb.png\" alt=\"QC Thumbnail\">\n",
    "        </div>\n",
    "        <div class=\"article-item\">\n",
    "            <h3><a href=\"/articles/article-3.html\">Sustainable Tech Innovations</a></h3>\n",
    "            <p>Innovations driving a greener future...</p>\n",
    "            <img src=\"/images/eco_tech_thumb.gif\" alt=\"Eco Tech Thumbnail\">\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <div class=\"hidden-info\" style=\"display:none;\">\n",
    "        This content is not visible but can be scraped.\n",
    "        <a href=\"/privacy\">Privacy Policy</a>\n",
    "    </div>\n",
    "\n",
    "    <img src=\"/banners/promo.jpg\" alt=\"Promotion Banner\">\n",
    "\n",
    "    <footer>\n",
    "        <p>Contact us at <a href=\"mailto:info@example.com\">info@example.com</a></p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Create a BeautifulSoup object for our examples\n",
    "soup = BeautifulSoup(sample_html, 'lxml') # Using lxml for performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa2838d",
   "metadata": {},
   "source": [
    "\n",
    "### Extracting Data from Tables üìà\n",
    "\n",
    "Tables are common structures for presenting tabular data on websites. Beautiful Soup makes it straightforward to extract this data row by row, cell by cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0936e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Extracting Data from Tables ---\")\n",
    "\n",
    "product_table = soup.find('table', id='product-table')\n",
    "if product_table:\n",
    "    headers = [th.get_text(strip=True) for th in product_table.find('thead').find_all('th')]\n",
    "    print(f\"Table Headers: {headers}\")\n",
    "\n",
    "    products_data = []\n",
    "    for row in product_table.find('tbody').find_all('tr'):\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) == len(headers): # Ensure row has correct number of cells\n",
    "            product = {headers[i]: cells[i].get_text(strip=True) for i in range(len(headers))}\n",
    "            products_data.append(product)\n",
    "            print(f\"Extracted: {product}\")\n",
    "    print(\"\\nAll products extracted from table:\")\n",
    "    for p in products_data:\n",
    "        print(p)\n",
    "else:\n",
    "    print(\"Product table not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533358bf",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "1.  **Locate the table:** `soup.find('table', id='product-table')` finds the table by its tag name and ID.\n",
    "2.  **Extract headers:**\n",
    "      * `product_table.find('thead')`: Finds the table header section.\n",
    "      * `.find_all('th')`: Finds all `<th>` (table header) tags within the `<thead>`.\n",
    "      * `[th.get_text(strip=True) for th in ...]`: A list comprehension to get the clean text from each header cell. `strip=True` removes leading/trailing whitespace.\n",
    "3.  **Extract rows and cells:**\n",
    "      * `product_table.find('tbody')`: Finds the table body.\n",
    "      * `.find_all('tr')`: Finds all `<tr>` (table row) tags within the `<tbody>`.\n",
    "      * `row.find_all('td')`: For each row, finds all `<td>` (table data/cell) tags.\n",
    "      * `{headers[i]: cells[i].get_text(strip=True) for i in range(len(headers))}`: Creates a dictionary for each product, mapping header names to cell values. This is a robust way to handle tabular data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460f289",
   "metadata": {},
   "source": [
    "\n",
    "### Scraping Links and Images üîó\n",
    "\n",
    "Extracting URLs from `<a>` (anchor) tags and `<img>` (image) tags is a very common task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69171e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping Links and Images ---\n",
      "Found 1 links:\n",
      " - Text: 'About Us', Href: 'https://example.com/about'\n",
      "\n",
      "Found 1 images:\n",
      " - Src: '/images/logo.png', Alt: 'Company Logo'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Scraping Links and Images ---\")\n",
    "\n",
    "# Scraping all links (<a> tags)\n",
    "all_links = soup.find_all('a')\n",
    "print(f\"Found {len(all_links)} links:\")\n",
    "for link in all_links:\n",
    "    href = link.get('href') # Get the value of the 'href' attribute\n",
    "    text = link.get_text(strip=True)\n",
    "    if href: # Only print if href exists\n",
    "        print(f\" - Text: '{text}', Href: '{href}'\")\n",
    "\n",
    "# Scraping all images (<img> tags)\n",
    "all_images = soup.find_all('img')\n",
    "print(f\"\\nFound {len(all_images)} images:\")\n",
    "for img in all_images:\n",
    "    src = img.get('src') # Get the value of the 'src' attribute\n",
    "    alt = img.get('alt', 'No Alt Text') # Get 'alt' attribute, with a default if not present\n",
    "    if src:\n",
    "        print(f\" - Src: '{src}', Alt: '{alt}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e32274",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `link.get('href')`: This is the safe way to access attributes. If the `href` attribute doesn't exist, `get()` returns `None`, preventing an error. You can also use `link['href']`, but that will raise a `KeyError` if the attribute is missing.\n",
    "  * `img.get('alt', 'No Alt Text')`: Shows how to provide a default value if an attribute is not found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cf4da",
   "metadata": {},
   "source": [
    "\n",
    "### Handling Nested Structures üå≥\n",
    "\n",
    "Web pages often have deeply nested HTML. Beautiful Soup's navigation methods (like `find()`, `find_all()`, `.children`, `.descendants`, and CSS selectors) are perfect for this.\n",
    "\n",
    "Let's extract information from the `article-item` divs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a8f6984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Handling Nested Structures ---\n",
      "Found 0 article items:\n",
      "\n",
      "All articles extracted:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Handling Nested Structures ---\")\n",
    "\n",
    "articles = []\n",
    "article_items = soup.find_all('div', class_='article-item')\n",
    "print(f\"Found {len(article_items)} article items:\")\n",
    "\n",
    "for item in article_items:\n",
    "    title_tag = item.find('h3').find('a') # Find <h3> then its child <a>\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "    link = title_tag.get('href') if title_tag else \"N/A\"\n",
    "\n",
    "    description_tag = item.find('p')\n",
    "    description = description_tag.get_text(strip=True) if description_tag else \"N/A\"\n",
    "\n",
    "    image_tag = item.find('img')\n",
    "    image_src = image_tag.get('src') if image_tag else \"N/A\"\n",
    "    image_alt = image_tag.get('alt', 'No Alt') if image_tag else \"N/A\"\n",
    "\n",
    "    article_info = {\n",
    "        'title': title,\n",
    "        'link': link,\n",
    "        'description': description,\n",
    "        'image_src': image_src,\n",
    "        'image_alt': image_alt\n",
    "    }\n",
    "    articles.append(article_info)\n",
    "    print(f\" - Extracted: {article_info['title']} ({article_info['link']})\")\n",
    "\n",
    "print(\"\\nAll articles extracted:\")\n",
    "for article in articles:\n",
    "    print(article)\n",
    "\n",
    "# Accessing content that is visually hidden but in HTML (useful for some cases)\n",
    "hidden_div = soup.find('div', class_='hidden-info')\n",
    "if hidden_div:\n",
    "    print(f\"\\nContent from hidden div: {hidden_div.get_text(strip=True)}\")\n",
    "    hidden_link = hidden_div.find('a')\n",
    "    if hidden_link:\n",
    "        print(f\"Hidden link: {hidden_link['href']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990d5b2",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * **Chaining `find()` calls:** `item.find('h3').find('a')` is a very common pattern. It first finds the `<h3>` tag within the current `article-item` and then finds the `<a>` tag *within that `<h3>`*. This ensures you get the `<a>` tag relevant to *that specific article*.\n",
    "  * **Error handling with `if title_tag else \"N/A\"`:** It's good practice to check if a `find()` call returned a tag (i.e., not `None`) before trying to access its attributes or text, to prevent `AttributeError`.\n",
    "  * **Scraping hidden content:** Beautiful Soup parses the entire HTML structure, so even elements styled with `display:none` or `visibility:hidden` in CSS will be available in the parse tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57501400",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Saving Scraped Data (CSV, JSON) üíæ\n",
    "\n",
    "Once you've extracted your data into Python lists and dictionaries, you'll want to save it in a structured format for later use or analysis. CSV and JSON are excellent choices.\n",
    "\n",
    "#### Saving to CSV\n",
    "\n",
    "CSV (Comma Separated Values) is a simple, common format for tabular data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82199a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-using 'products_data' from the table extraction example\n",
    "if products_data:\n",
    "    csv_filename = 'products.csv'\n",
    "    # Determine fieldnames (headers) from the first dictionary\n",
    "    fieldnames = list(products_data[0].keys())\n",
    "\n",
    "    print(f\"\\n--- Saving products data to {csv_filename} ---\")\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader() # Write the header row\n",
    "        writer.writerows(products_data) # Write all product data rows\n",
    "    print(f\"Successfully saved {len(products_data)} products to {csv_filename}\")\n",
    "else:\n",
    "    print(\"\\nNo product data to save to CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63936824",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `import csv`: Imports the `csv` module.\n",
    "  * `fieldnames = list(products_data[0].keys())`: Gets the column headers from the keys of the first product dictionary. Assumes all dictionaries have the same keys.\n",
    "  * `with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:`: Opens the CSV file in write mode.\n",
    "      * `newline=''`: Important for CSV writing to prevent extra blank rows.\n",
    "      * `encoding='utf-8'`: Ensures proper handling of various characters.\n",
    "  * `csv.DictWriter(csvfile, fieldnames=fieldnames)`: Creates a `DictWriter` object, which maps dictionaries to rows.\n",
    "  * `writer.writeheader()`: Writes the first row using the `fieldnames`.\n",
    "  * `writer.writerows(products_data)`: Writes all dictionaries in the `products_data` list as rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65603f7",
   "metadata": {},
   "source": [
    "\n",
    "#### Saving to JSON\n",
    "\n",
    "JSON (JavaScript Object Notation) is a lightweight data-interchange format, great for hierarchical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-using 'articles' data from the nested structures example\n",
    "if articles:\n",
    "    json_filename = 'articles.json'\n",
    "    print(f\"\\n--- Saving articles data to {json_filename} ---\")\n",
    "    with open(json_filename, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(articles, jsonfile, indent=4, ensure_ascii=False)\n",
    "    print(f\"Successfully saved {len(articles)} articles to {json_filename}\")\n",
    "else:\n",
    "    print(\"\\nNo article data to save to JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93a378",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `import json`: Imports the `json` module.\n",
    "  * `json.dump(articles, jsonfile, indent=4, ensure_ascii=False)`: Writes the `articles` list (of dictionaries) to the JSON file.\n",
    "      * `indent=4`: Makes the JSON output pretty-printed with 4-space indentation, making it human-readable.\n",
    "      * `ensure_ascii=False`: Ensures that non-ASCII characters (like special symbols) are written as is, not as `\\uXXXX` escape sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa7703",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "#### ‚ùì Quick Quiz: Practical Beautiful Soup Applications\n",
    "\n",
    "1.  When extracting data from an `<a>` tag, which attribute typically holds the URL?\n",
    "\n",
    "      * A) `src`\n",
    "      * B) `href`\n",
    "      * C) `link`\n",
    "      * D) `url`\n",
    "\n",
    "2.  To extract the text content of a `<td>` tag within a table row, what is the recommended way to remove leading/trailing whitespace?\n",
    "\n",
    "      * A) `cell.text.strip()`\n",
    "      * B) `cell.get_text()`\n",
    "      * C) `cell.get_text(clean=True)`\n",
    "      * D) `cell.get_text(strip=True)`\n",
    "\n",
    "3.  If you've scraped a list of dictionaries and want to save it as a CSV file, which module would you primarily use in Python?\n",
    "\n",
    "      * A) `json`\n",
    "      * B) `pandas`\n",
    "      * C) `csv`\n",
    "      * D) `io`\n",
    "\n",
    "*(Answers are at the end of the section\\!)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251da31",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "# 3.7 Advanced Beautiful Soup Techniques\n",
    "\n",
    "Let's briefly touch upon some more advanced concepts and how Beautiful Soup fits into larger, more complex scraping pipelines.\n",
    "\n",
    "### Dealing with Common Scraping Challenges üöß\n",
    "\n",
    "Web scraping isn't always smooth sailing. Here are common hurdles and approaches:\n",
    "\n",
    "  * **Dynamic Content (JavaScript-rendered pages):**\n",
    "      * **Challenge:** Many modern websites load content dynamically using JavaScript *after* the initial HTML loads. `requests` only gets the initial HTML, not what JavaScript renders.\n",
    "      * **Solution (Brief Mention):** Beautiful Soup *cannot* execute JavaScript. For these sites, you need a headless browser automation tool like `Selenium` or `Playwright`. These tools launch a real browser instance (without a visible GUI), allow the page to fully render, and then you can pass the rendered HTML to Beautiful Soup for parsing.\n",
    "  * **Anti-Scraping Measures:**\n",
    "      * **Challenge:** Websites employ various techniques to deter scrapers (e.g., blocking IPs, checking User-Agents, CAPTCHAs, honeypot traps, complex AJAX requests).\n",
    "      * **Solutions (Brief Mention):**\n",
    "          * **Rotate User-Agents:** Mimic different browsers.\n",
    "          * **Proxies:** Route your requests through different IP addresses to avoid IP bans.\n",
    "          * **Delays:** As discussed, respect rate limits with `time.sleep()`.\n",
    "          * **CAPTCHA Solving Services:** For very aggressive sites (ethical implications here).\n",
    "          * **Headers:** Set realistic `Accept-Language`, `Referer`, etc., headers.\n",
    "          * **Session Handling:** Use `requests.Session()` to persist cookies and headers across requests, mimicking a real user session.\n",
    "      * **Important Note:** Overcoming anti-scraping measures can quickly become an arms race and has ethical and legal implications. Always evaluate if the data is genuinely public and if there's a less intrusive way (like an API).\n",
    "\n",
    "### Using `lxml` Parser for Speed ‚ö°\n",
    "\n",
    "As mentioned earlier, Beautiful Soup supports different parsers. `lxml` is written in C and is significantly faster and more robust at handling malformed HTML than Python's built-in `html.parser`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1722bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Let's create a very large HTML string to demonstrate speed difference\n",
    "large_html = \"<html><body>\" + \"<div><p>Some content</p></div>\" * 10000 + \"</body></html>\"\n",
    "\n",
    "print(\"\\n--- Comparing Parser Speed (lxml vs html.parser) ---\")\n",
    "\n",
    "start_time = time.time()\n",
    "soup_html_parser = BeautifulSoup(large_html, 'html.parser')\n",
    "end_time = time.time()\n",
    "print(f\"Parsing with 'html.parser' took: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "soup_lxml_parser = BeautifulSoup(large_html, 'lxml')\n",
    "end_time = time.time()\n",
    "print(f\"Parsing with 'lxml' took: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "# You'll usually see lxml being noticeably faster for large documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef821ac",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `BeautifulSoup(html_content, 'lxml')`: Explicitly tells Beautiful Soup to use the `lxml` parser.\n",
    "  * The example creates a large HTML string to make the performance difference more apparent.\n",
    "\n",
    "**Recommendation:** Always use `lxml` if you have it installed and are dealing with potentially large or complex HTML documents, or if performance is critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e614b6b",
   "metadata": {},
   "source": [
    "\n",
    "### Integrating with Other Libraries (e.g., `pandas` for Data Analysis) ü§ù\n",
    "\n",
    "Scraped data is raw data. To analyze it, visualize it, or prepare it for machine learning, you'll often integrate with other powerful Python libraries, especially `pandas`.\n",
    "\n",
    "`pandas` is excellent for data manipulation and analysis, and it can easily import data from lists of dictionaries (which is often the format scraped data ends up in).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a986d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Re-using 'products_data' from earlier table extraction\n",
    "# If you run this block independently, make sure products_data is defined.\n",
    "# For demo, let's create a simplified version:\n",
    "products_data_for_pd = [\n",
    "    {'Product Name': 'Smartphone X', 'Category': 'Electronics', 'Price': '$799.99', 'Availability': 'In Stock'},\n",
    "    {'Product Name': 'Laptop Pro', 'Category': 'Electronics', 'Price': '$1299.00', 'Availability': 'Low Stock'},\n",
    "    {'Product Name': 'Wireless Headphones', 'Category': 'Audio', 'Price': '$149.50', 'Availability': 'In Stock'},\n",
    "    {'Product Name': 'Smartwatch S', 'Category': 'Wearables', 'Price': '$299.00', 'Availability': 'Out of Stock'}\n",
    "]\n",
    "\n",
    "if products_data_for_pd:\n",
    "    print(\"\\n--- Integrating with Pandas for Data Analysis ---\")\n",
    "\n",
    "    # Create a Pandas DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(products_data_for_pd)\n",
    "    print(\"DataFrame created from scraped data:\")\n",
    "    print(df)\n",
    "\n",
    "    # Example: Basic data cleaning/transformation with Pandas\n",
    "    # Convert 'Price' column to numeric (remove '$', convert to float)\n",
    "    df['Price'] = df['Price'].replace({'\\$': ''}, regex=True).astype(float)\n",
    "    print(\"\\nDataFrame after converting 'Price' to numeric:\")\n",
    "    print(df)\n",
    "\n",
    "    # Example: Basic data analysis\n",
    "    avg_price = df['Price'].mean()\n",
    "    print(f\"\\nAverage Product Price: ${avg_price:.2f}\")\n",
    "\n",
    "    # Count availability status\n",
    "    availability_counts = df['Availability'].value_counts()\n",
    "    print(\"\\nAvailability Counts:\")\n",
    "    print(availability_counts)\n",
    "\n",
    "    # Filter products\n",
    "    in_stock_products = df[df['Availability'] == 'In Stock']\n",
    "    print(\"\\nIn Stock Products:\")\n",
    "    print(in_stock_products[['Product Name', 'Price']])\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo product data to demonstrate with Pandas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb513d24",
   "metadata": {},
   "source": [
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "  * `import pandas as pd`: Imports the `pandas` library.\n",
    "  * `df = pd.DataFrame(products_data_for_pd)`: The easiest way to create a DataFrame from a list of dictionaries. Each dictionary becomes a row, and the keys become column headers.\n",
    "  * **Data Cleaning:** `df['Price'].replace({'\\$': ''}, regex=True).astype(float)` demonstrates a common data cleaning step: removing unwanted characters (`$`) and converting the column to a numeric type (`float`).\n",
    "  * **Data Analysis:** `df['Price'].mean()`, `df['Availability'].value_counts()`, and `df[df['Availability'] == 'In Stock']` show simple aggregation and filtering operations that are trivial with Pandas.\n",
    "\n",
    "**Key takeaway:** Web scraping often serves as the data collection phase. Once collected, `pandas` is your next best friend for transforming, analyzing, and preparing that data for further insights or machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf482b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "587f7888",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### Quick Quiz Answers: Advanced Beautiful Soup Techniques\n",
    "\n",
    "1.  If a website's content is primarily loaded via JavaScript *after* the initial page loads, which tool would you most likely need in addition to `requests` and Beautiful Soup to scrape that content?\n",
    "\n",
    "      * A) `Scrapy`\n",
    "      * B) `Selenium` (or Playwright)\n",
    "      * C) `Flask`\n",
    "      * D) `Django`\n",
    "\n",
    "    *Correct Answer: B) `Selenium` (or Playwright)*\n",
    "\n",
    "2.  What is the primary benefit of using the `lxml` parser with Beautiful Soup compared to `html.parser`?\n",
    "\n",
    "      * A) It automatically handles CAPTCHAs.\n",
    "      * B) It is generally faster and more robust for malformed HTML.\n",
    "      * C) It allows you to execute JavaScript directly.\n",
    "      * D) It includes built-in proxy rotation.\n",
    "\n",
    "    *Correct Answer: B) It is generally faster and more robust for malformed HTML.*\n",
    "\n",
    "3.  After scraping tabular data into a list of dictionaries, which Python library is typically used to easily perform operations like calculating averages, filtering rows, or summarizing data?\n",
    "\n",
    "      * A) `numpy`\n",
    "      * B) `matplotlib`\n",
    "      * C) `pandas`\n",
    "      * D) `scipy`\n",
    "\n",
    "    *Correct Answer: C) `pandas`*\n",
    "\n",
    "-----\n",
    "\n",
    "## Congratulations\\! üéâ\n",
    "\n",
    "You've successfully completed **Section 3: Web Scraping with Beautiful Soup**\\! You've learned:\n",
    "\n",
    "  * The definition and use cases of web scraping.\n",
    "  * **Crucial ethical and legal considerations** (`robots.txt`, ToS, rate limiting). This is paramount\\!\n",
    "  * How to make **HTTP requests** using the `requests` library (GET, POST, headers, parameters, error handling).\n",
    "  * The **fundamentals of Beautiful Soup** for parsing HTML/XML.\n",
    "  * Powerful methods for **searching the parse tree** (`find()`, `find_all()`, CSS selectors, regex).\n",
    "  * How to **modify the parse tree** (adding, removing, changing elements).\n",
    "  * **Practical applications** like extracting tables, links, and images, and handling nested structures.\n",
    "  * How to **save scraped data** to CSV and JSON files.\n",
    "  * Briefly touched upon **advanced challenges** (dynamic content, anti-scraping) and the benefits of **`lxml` and `pandas` integration**.\n",
    "\n",
    "Web scraping is an incredibly powerful skill, but with great power comes great responsibility. Always scrape ethically and legally\\!\n",
    "\n",
    "You now have a solid foundation to start building your own web scrapers. The best way to solidify this knowledge is to practice\\! Find a simple website (with a permissive `robots.txt` and ToS, perhaps one designed for practice like `http://books.toscrape.com/`) and try to extract some data yourself.\n",
    "\n",
    "Keep exploring, keep building, and happy scraping\\! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
